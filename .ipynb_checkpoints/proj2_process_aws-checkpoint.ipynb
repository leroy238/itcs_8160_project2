{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting boto3\n",
      "  Downloading boto3-1.35.76-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting botocore<1.36.0,>=1.35.76 (from boto3)\n",
      "  Downloading botocore-1.35.76-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3)\n",
      "  Downloading s3transfer-0.10.4-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\yarka\\anaconda3\\envs\\databases\\lib\\site-packages (from botocore<1.36.0,>=1.35.76->boto3) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in c:\\users\\yarka\\anaconda3\\envs\\databases\\lib\\site-packages (from botocore<1.36.0,>=1.35.76->boto3) (2.2.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\yarka\\anaconda3\\envs\\databases\\lib\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.36.0,>=1.35.76->boto3) (1.16.0)\n",
      "Downloading boto3-1.35.76-py3-none-any.whl (139 kB)\n",
      "Downloading botocore-1.35.76-py3-none-any.whl (13.2 MB)\n",
      "   ---------------------------------------- 0.0/13.2 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 2.1/13.2 MB 14.7 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 5.5/13.2 MB 15.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 8.7/13.2 MB 15.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 12.1/13.2 MB 15.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.2/13.2 MB 14.8 MB/s eta 0:00:00\n",
      "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Downloading s3transfer-0.10.4-py3-none-any.whl (83 kB)\n",
      "Installing collected packages: jmespath, botocore, s3transfer, boto3\n",
      "Successfully installed boto3-1.35.76 botocore-1.35.76 jmespath-1.0.1 s3transfer-0.10.4\n"
     ]
    }
   ],
   "source": [
    "# installing boto 3\n",
    "! pip install boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Connection Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import boto3\n",
    "import shutil\n",
    "import glob\n",
    "import datetime as dt\n",
    "from datetime import date, datetime, timedelta\n",
    "\n",
    "\n",
    "bucket_name = 'Your Teams Bucket Name'\n",
    "\n",
    "enter_aws_access_key_id ='Access Key' \n",
    "\n",
    "enter_aws_secret_access_key ='Secret Access Key'\n",
    "\n",
    "#NO SPACE between group & number\n",
    "group_number = 'group13' \n",
    "\n",
    "# path to the folder to save NO SAPCE in folder name\n",
    "save_folder_path = os.getcwd()\n",
    "\n",
    "# the folder names in the data folder your group members names as apear on AWS S3\n",
    "#for example\n",
    "participant_list = ['ParticipantId-1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3',\n",
    "                 aws_access_key_id= enter_aws_access_key_id,\n",
    "                 aws_secret_access_key= enter_aws_secret_access_key)\n",
    "\n",
    "\n",
    "#     ## Bucket to use\n",
    "bucket = s3.Bucket(bucket_name)\n",
    "\n",
    "\n",
    "#get participant list\n",
    "def get_participant_list(bucket_input):\n",
    "    client = boto3.client('s3',\n",
    "                     aws_access_key_id= enter_aws_access_key_id,\n",
    "                     aws_secret_access_key= enter_aws_secret_access_key)\n",
    "    result = client.list_objects(Bucket=bucket_input, Prefix='data/', Delimiter='/')\n",
    "    part_list = []\n",
    "    for o in result.get('CommonPrefixes'):\n",
    "        part_list.append(o.get('Prefix').split('/')[1])\n",
    "    return part_list\n",
    "\n",
    "#update this to the correct format\n",
    "\n",
    "\n",
    "#path of the files\n",
    "file_list=[]\n",
    "\n",
    "\n",
    "def get_objects(part_name):\n",
    "     for obj in bucket.objects.filter(Delimiter='/', Prefix=f'data/{part_name}/'):\n",
    "         file_list.append(obj.key)\n",
    "\n",
    "for i in range(len(participant_list)):\n",
    "     get_objects(participant_list[i])\n",
    "\n",
    "def download_dir(prefix, local, bucket):\n",
    "    client = boto3.client('s3',\n",
    "                 aws_access_key_id= enter_aws_access_key_id,\n",
    "                 aws_secret_access_key= enter_aws_secret_access_key)\n",
    "    keys = []\n",
    "    dirs = []\n",
    "    next_token = ''\n",
    "    base_kwargs = {\n",
    "        'Bucket':bucket,\n",
    "        'Prefix':prefix,\n",
    "    }\n",
    "    while next_token is not None:\n",
    "        kwargs = base_kwargs.copy()\n",
    "        if next_token != '':\n",
    "            kwargs.update({'ContinuationToken': next_token})\n",
    "        results = client.list_objects_v2(**kwargs)\n",
    "        contents = results.get('Contents')\n",
    "        for i in contents:\n",
    "            k = i.get('Key')\n",
    "            if k[-1] != '/':\n",
    "                keys.append(k)\n",
    "            else:\n",
    "                dirs.append(k)\n",
    "        next_token = results.get('NextContinuationToken')\n",
    "    for d in dirs:\n",
    "        dest_pathname = os.path.join(local, d)\n",
    "        if not os.path.exists(os.path.dirname(dest_pathname)):\n",
    "            os.makedirs(os.path.dirname(dest_pathname))\n",
    "    for k in keys:\n",
    "        dest_pathname = os.path.join(local, k)\n",
    "        if not os.path.exists(os.path.dirname(dest_pathname)):\n",
    "            os.makedirs(os.path.dirname(dest_pathname))\n",
    "        client.download_file(bucket, k, dest_pathname)\n",
    "\n",
    "\n",
    "for i in range(len(participent_list)):\n",
    "    download_dir(('data/'+participent_list[i]),save_folder_path,bucket_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the data set and output CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the dataset function\n",
    "def create_combined_dataset(path_to_raw_data_folder):\n",
    "    data = [pd.read_json(f'{path_to_raw_data_folder}/{i}') for i in os.listdir(path_to_raw_data_folder) if i.endswith('.gz')]\n",
    "    return pd.concat(data)\n",
    "\n",
    "# Function to fix timestamp from UTC to EST\n",
    "def fix_time(a):\n",
    "    b = pd.Timestamp(a)\n",
    "    c = b.tz_convert(\"America/New_York\")\n",
    "    d = pd.to_datetime(c)\n",
    "    e = d.to_pydatetime().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    return(e)\n",
    "\n",
    "\n",
    "try:\n",
    "    os.mkdir(save_folder_path+'/data/allfiles')\n",
    "except:\n",
    "    pass\n",
    "#Move all the results into a single folder\n",
    "for j in range(len(participent_list)):\n",
    "    source_dir = f'{save_folder_path}/data/{participent_list[j]}'\n",
    "\n",
    "    target_dir = save_folder_path +'/data/allfiles'\n",
    "\n",
    "    file_names = os.listdir(source_dir)\n",
    "\n",
    "    for file_name in file_names:\n",
    "        shutil.copy(os.path.join(source_dir, file_name), target_dir)\n",
    "\n",
    "# find corrupt files, move them to a corrupt folder\n",
    "zip_file_names = list(glob.glob(target_dir+\"/*.gz\"))\n",
    "corrupt_files = []\n",
    "ok_files_count=0\n",
    "corrupt_file_count = 0\n",
    "for i in range(len(zip_file_names)):\n",
    "    try:\n",
    "        pd.read_json(zip_file_names[i])\n",
    "        ok_files_count += 1  \n",
    "        #print(zip_file_names[i] +' ---> FILE OK')\n",
    "    except:\n",
    "        corrupt_files.append(zip_file_names[i])\n",
    "        #print(zip_file_names[i]+' ---> CORRUPTED')\n",
    "        corrupt_file_count += 1\n",
    "\n",
    "################################ folders for files \n",
    "try:\n",
    "    os.mkdir(save_folder_path+'/data/corrupt_file_folder')\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    for i in range(len(corrupt_files)):\n",
    "        shutil.move(f\"{corrupt_files[i]}\", f\"{save_folder_path}/data/corrupt_file_folder\")        \n",
    "except:\n",
    "    pass\n",
    "\n",
    "df = create_combined_dataset(target_dir)        \n",
    "\n",
    "df.rename(columns={'$type':'Type'},inplace=True) #fixing the name\n",
    "\n",
    "df.rename(columns={'Level':'BatteryLevel'},inplace=True) #fixing the level name\n",
    "\n",
    "df.rename(columns={'Accuracy':'LocationAccuracy'},inplace=True) #fixing the accuracy name\n",
    "\n",
    "# this is a metadata column that can remove\n",
    "try:\n",
    "    df.drop('ProbeParticipation', axis=1, inplace=True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "# If the column protocol name has the protocol name we remove it since every\n",
    "# participent is in the same study\n",
    "try:\n",
    "    df= df[df['ProtocolName'].isnull()] #removing info row\n",
    "except:\n",
    "    pass\n",
    "\n",
    "#\n",
    "df.index = pd.RangeIndex(len(df.index)) #fixing the index\n",
    "df.index.name = 'Row_id' #adding a new id row\n",
    "df['Formatted_time'] = df['Timestamp'].apply(lambda x: fix_time(x)) # adding a new column changing UTC to EST\n",
    "df['Formatted_time'] = pd.to_datetime(df['Formatted_time'] ) # fix data type\n",
    "\n",
    "\n",
    "# remove extra\n",
    "df['Type'] = df['Type'].replace(regex = {'Sensus.Participation':''})\n",
    "df['Type'] = df['Type'].replace(regex = {' SensusiOS':''})\n",
    "df['Type'] = df['Type'].replace(regex = {'Sensus.Probes.Device.': ''})\n",
    "df['Type'] = df['Type'].replace(regex = {'Sensus.Probes.Location.': ''})\n",
    "df['Type'] = df['Type'].replace(regex = {'Sensus.Probes.Movement.':''})\n",
    "df['Type'] = df['Type'].replace(regex = {'Sensus.Probes.Network.': ''})\n",
    "df['Type'] = df['Type'].replace(regex = {'Sensus.Probes.Context.': ''})\n",
    "df['Type'] = df['Type'].replace(regex = {'Datum,':''})\n",
    "\n",
    "# removing metadata rows\n",
    "df = df[df['Type'] != 'Sensus.Heartbeat']\n",
    "df = df[df['Type'] != 'Report']\n",
    "\n",
    "### fixing the values for mysql\n",
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'],utc=True)\n",
    "df['Formatted_time'] = pd.to_datetime(df['Formatted_time'] )\n",
    "\n",
    "# fixing the order\n",
    "try:\n",
    "    df = df[['Type','Id','DeviceId','Timestamp','ProtocolId','BuildId','ParticipantId','DeviceManufacturer',\n",
    "         'DeviceModel','OperatingSystem','TaggedEventId','TaggedEventTags','SensingAgentStateDescription',\n",
    "         'LocalOffsetFromUTC','Decibels','BatteryLevel','AccessPointBSSID','Activity','Phase','State',\n",
    "         'Confidence','Latitude','Longitude','LocationAccuracy','ProtocolName','Formatted_time']]\n",
    "except:\n",
    "    df = df[['Type','Id','DeviceId','Timestamp','ProtocolId','BuildId','ParticipantId','DeviceManufacturer',\n",
    "         'DeviceModel','OperatingSystem','TaggedEventId','TaggedEventTags','SensingAgentStateDescription',\n",
    "         'LocalOffsetFromUTC','Decibels','BatteryLevel','AccessPointBSSID','Activity','Phase','State',\n",
    "         'Confidence','ProtocolName','Formatted_time']]\n",
    "\n",
    "df.to_csv(f'{save_folder_path}/{group_number}_output.csv')\n",
    "\n",
    "shutil.rmtree(f'{save_folder_path}/data/allfiles')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
